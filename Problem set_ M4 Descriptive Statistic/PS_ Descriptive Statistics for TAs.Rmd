---
title: "PS_ Descriptive Statistics for TAs"
author: "Pablo E. Guti√©rrez-Fonseca"
date: "2023-08-13"
output: pdf_document
---




# **R practice.<br />**
\vspace{1em}

**Install packages.<br />**  
```{r}
library(ggplot2)
library(moments)
```
\vspace{1em}

**Load the water pollution data into R.<br />** 
```{r, echo=FALSE}
df_snow <- read.csv('C:/Users/Guti/OneDrive - University of Vermont/Curriculum/07_ Cursos/NR-2400/Problem set_ M4 Descriptive Statistic/Snow Depth data.csv')
summary(df_snow)

df_trees <- read.csv('C:/Users/Guti/OneDrive - University of Vermont/Curriculum/07_ Cursos/NR-2400/Problem set_ M4 Descriptive Statistic/Boardstrength.csv')
summary(df_trees)
```
\newpage

1. the **MEAN Yearly Mean Snow Depth**.  
```{r}
mean(df_snow$Mean_Dec_May_snow_depth)
```
\vspace{1em}

2. the **MEDIAN Yearly Mean Snow Depth**.  
```{r}
median(df_snow$Mean_Dec_May_snow_depth)
```
\vspace{1em}

3. If you round **Yearly Mean Snow Depth** to the nearest integer (whole number with no
decimals), what is the MODE? 
R does not have a standard in-built function to calculate mode. So we create a user function to calculate mode of a data set in R. This function takes the vector as input and gives the mode value as output.
```{r}
# Round the continuous column "value" to 1 decimal place
df_snow$Mean_Dec_May_snow_depth <- round(df_snow$Mean_Dec_May_snow_depth, 0)

calculate_mode <- function(x) {
  uniq_x <- unique(x)
  freq <- tabulate(match(x, uniq_x))
  uniq_x[which.max(freq)]
}

# Calculate the mode of the rounded column
mode_value <- calculate_mode(df_snow$Mean_Dec_May_snow_depth)
# Print the mode
print(paste("Mode:", mode_value))
```
\vspace{1em}

4. Still in Excel, now calculate the STANDARD DEVIATION for the Yearly Mean Snow
Depth.
```{r}
sd(df_snow$Mean_Dec_May_snow_depth)
```
\vspace{1em}

5. What is the **INTER-QUARTILE RANGE** for **Yearly Mean snow Depth**?
```{r}
# Calculate the IQR for the specified column
iqr_value <- IQR(df_snow$Mean_Dec_May_snow_depth)

# Print the IQR
print(iqr_value)
```
\vspace{1em}

6. Using the Interquartile range technique, how many **OUTLIER** years are there in your
Yearly Mean Snow Depth Data? 
```{r}
# Calculate the quartiles and IQR
q1 <- quantile(df_snow$Mean_Dec_May_snow_depth, 0.25, na.rm = TRUE)
q3 <- quantile(df_snow$Mean_Dec_May_snow_depth, 0.75, na.rm = TRUE)
iqr <- q3 - q1

# Define the lower and upper bounds for outliers
lower_bound <- q1 - 1.5 * iqr
upper_bound <- q3 + 1.5 * iqr

# Identify outlier years
outlier_years <- df_snow$Mean_Dec_May_snow_depth < lower_bound | df_snow$Mean_Dec_May_snow_depth > upper_bound

# Count the number of outlier years
num_outliers <- sum(outlier_years)

# Print the number of outlier years
print(num_outliers)
```
\vspace{1em}

8. Now calculate **Pearson's skew for the Yearly Mean Snow Depth Data**. Enter your
answer rounded to 2 decimal places. (do this in JMP .....you've already proved your excel
skills above) 
```{r}
# Calculate Pearson's skew
skew <- skewness(df_snow$Mean_Dec_May_snow_depth, na.rm = TRUE)

# Print the skew rounded to 2 decimal places
print(round(skew, 2))
```
\vspace{1em}

9. Calculate the **standard error of skew (ses)** for this data so we can determine the
significance of our skew. 
```{r}
# Calculate the skewness and standard error of skew
n <- length(df_snow$Mean_Dec_May_snow_depth)
se_skew <- sqrt(6 * n * (n - 1) / ((n - 2) * (n + 1) * (n + 3)))

# Print the skew and standard error of skew
cat("Skewness:", skew)
cat("Standard Error of Skewness:", se_skew)
```
\vspace{1em}

10. Based on the ses technique, is this **skew significant**?  
**No**
\vspace{1em}

11. Now determine the **kurtosis for the Yearly Mean Snow Depth data**. 
```{r}
# Calculate the kurtosis
kurt <- kurtosis(df_snow$Mean_Dec_May_snow_depth, na.rm = TRUE)

# Print the kurtosis
cat("Kurtosis :", kurt)
```
\vspace{1em}

12. Calculate the **standard error of kurtosis (sek)** for this data so we can determine the significance of our skew. 
```{r}
# Calculate the kurtosis and sample size
kurt <- kurtosis(df_snow$Mean_Dec_May_snow_depth, na.rm = TRUE)
n <- length(df_snow$Mean_Dec_May_snow_depth)

# Calculate the standard error of kurtosis
sek <- sqrt((24 * n * (n - 2) * (n - 3)) / ((n + 1) * (n + 1) * (n + 3) * (n + 5))) * (1 - (6 / (n + 1) + 1 / (n + 3)))

# Print the kurtosis and standard error of kurtosis
cat("Kurtosis:", kurt)
cat("Standard Error of Kurtosis:", sek)
```
\vspace{1em}

13. Based on the sek technique, is this **kurtosis significant**? 
**No**
\vspace{1em}

14. Based on your statistical summary values, do you think that your data is normally
distributed?
**No**
\vspace{1em}

15. Is your data normally distributed? How can you tell? Be sure to report your certainty of this conclusion.
```{r}
shapiro.test(df_snow$Mean_Dec_May_snow_depth)
```
\vspace{1em}

16. Based on these results, which measure of central tendency would be best describe central tendency for this dataset?
**Mean**
\vspace{1em}

17. Now use the **Boardstrength data** down-loadable from this blackboard folder. This
datafile contains measurements of board strength for several different types of
wood. Your task is to describe this data (**Strength variable**) as though you were
presenting it in a paper, poster or presentation.
```{r}
# Calculate mean and standard deviation
mean_strength <- mean(df_trees$Strength, na.rm = TRUE)
sd_strength <- sd(df_trees$Strength, na.rm = TRUE)

# Calculate quartiles (25th, 50th, 75th percentiles)
quartiles <- quantile(df_trees$Strength, probs = c(0.25, 0.5, 0.75), na.rm = TRUE)

# Test for normality using Shapiro-Wilk test
normality_test <- shapiro.test(df_trees$Strength)

# Calculate median
median_strength <- median(df_trees$Strength, na.rm = TRUE)

# Calculate mode using a custom function
calculate_mode <- function(x) {
  uniq_x <- unique(x)
  freq <- tabulate(match(x, uniq_x))
  uniq_x[which.max(freq)]
}
mode_strength <- calculate_mode(df_trees$Strength)

# Identify outlier years using IQR technique
q1 <- quantile(df_trees$Strength, 0.25, na.rm = TRUE)
q3 <- quantile(df_trees$Strength, 0.75, na.rm = TRUE)
iqr <- q3 - q1
lower_bound <- q1 - 1.5 * iqr
upper_bound <- q3 + 1.5 * iqr
outlier_years <- df_trees$Strength < lower_bound | df_trees$Strength > upper_bound
num_outliers <- sum(outlier_years)

# Print the results
print(paste("Mean:", mean_strength))
print(paste("Standard Deviation:", sd_strength))
print("Quartiles:")
print(quartiles)
print("Shapiro-Wilk Test for Normality:")
print(normality_test)
print(paste("Median:", median_strength))
print(paste("Mode:", mode_strength))
print(paste("Number of Outliers:", num_outliers))
```
\vspace{1em}

```{r}
ggplot(df_trees, aes(x=Type, y=Strength)) +
  geom_boxplot()
```


18. If you wanted to highlight the differences between species, and describe strength for
each of them, how would your table look different? Consider that the actual statistical
analysis to test for differences among species is run on the full sample of 20 observations, so metrics like kurtosis, skew and goodness of fit tests would be calculated on the full data set. 
